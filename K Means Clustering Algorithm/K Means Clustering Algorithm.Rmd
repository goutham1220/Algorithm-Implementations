---
title: *k*"-Means Clustering Algorithm"
author: "Goutham Swaminathan"
output: pdf_document
---

#Introduction
 The *k*-Means Clustering Algorithm is an implementation of *k*-means clustering, a statistical method to partition data points into clusters based on their distance away from the cluster's mean (need to reword this). The *k*-means clustering algorithm is useful for predictive categorization of similar data points (this is probably wrong or poorly worded so I have to fix this).

#Explanation
 In order to implement this algorithm and apply it to a dataset, we will need to do the following:
  1.  Create a distance function that calculates the distance between a point and a centroid
  2.  Assign a random point in the dataset to every centroid
  3.  Apply the distance function to every point and every centroid and record the results
  4.  Assign each point to a cluster based on the minimum distance between the point and a centroid
  5.  Reassign each centroid to the mean distance of its respective cluster
  6.  Rerun the algorithm until mean distance is minimized 
 
```{r}
numPoints = 200
numClusters.data = 5
numClusters = 5
numIterations = 10
spread = 4
```

We first initialize variables for the number of points, number of inherent clusters in the data, number of desired clusters, number of iterations, and the spread of the data. Initializing these variables allows for the most customization of the algorithm when testing it with data. After

We will then create a *distance()* function to calculate the distance between every point in a matrix and a centroid. We will first initialize the **dist** variable which will hold a matrix of distances, with each row containing the distances for a different centroid. Using the *apply()* function, we will set the **X** parameter to **matrix** and the **MARGIN** parameter to 1 (because we want to apply the function across every row). We will then create our own function with the *apply()* function, in which we set the **centroid** variable to the first two elements of the **centroid** vector. This is due to the fact that later in the program, we will use the 3rd column of the **centroids** matrix to hold the color of each centroid. In addition, this **centroid** variable is a vector rather than a matrix because in our *kmeans_simulation()* function, we will apply this function to every row of the **centroids** matrix. Afterwards, we will calculate the total square difference between the *x* and *y* values for the **centroid** and **x** variables. Finally, we will return the **dist** vector.

```{r}
distance <- function(matrix, centroid){
  
  dist = apply(matrix, MARGIN = 1, FUN = function(x){
     centroid = centroid[1:2]
     sqrt(sum((centroid - x)^2))})
  
  return(dist)
}
```

This *distance()* function will become very important in our *kmeans_simulation()* function and creating it effectively accomplishes the first step (this sounds kind of dumb).

```{r}
kmeans_simulation = function(npoints, nclusters, niterations, nclusters.data = 4, spread = 5){
```

Now, we 

```{r}
  xvalues = vector()
  yvalues = vector()
  
  for(i in 1:nclusters.data){
    xvalues = c(xvalues, rnorm(npoints/nclusters.data, mean = (25*i), sd = spread))
    yvalues = c(yvalues, rnorm(npoints/nclusters.data, mean = (100*i)/nclusters.data, sd = spread))
  }
  
  dataset = cbind(xvalues, yvalues)

  # initialize centroids to random values
  # xy =  rnorm(nclusters * 2, mean = meanValue, sd = spread)
  xy =  dataset[sample(1:npoints, nclusters),]
  color = 1:nrow(xy)
  centroids = cbind(xy, color)
  
  centroids_init = centroids
  cost = vector()
  
  for(j in 1:niterations){
    
    alldistances = apply(centroids, MARGIN = 1, distance, matrix = dataset)
    clusterAssignment = apply(alldistances, MARGIN = 1, which.min)
    cost[j] = sum(sapply(1:nclusters, 
                         function(x) sum(distance(dataset[clusterAssignment == x,], centroids[x,]))))
    centroids[,1:2] = sapply(1:nclusters, 
                             function(x) mean(dataset[clusterAssignment == x,1:2]))

  }
  
  return (list(cost, clusterAssignment, centroids, dataset, centroids_init))
}
```
```{r}
list = kmeans_simulation(numPoints, numClusters, numIterations, numClusters.data, spread)

ggplot(data = NULL) +
  geom_point(size=2, shape="23") + xlab("X") + ylab("Y") +
  geom_point(aes(x=list[[5]][,1], y=list[[5]][,2]), colour=list[[5]][,3], size=5) +
  geom_point(aes(x=list[[4]][,1], y=list[[4]][,2]), colour="black",fill = "white", pch=21, size=2)

Sys.sleep(2)

ggplot(data = NULL) +
  geom_point(aes(x=list[[3]][,1], y=list[[3]][,2]), colour=list[[3]][,3], size=5) +
  geom_point(aes(x=list[[4]][,1], y=list[[4]][,2]), colour=list[[2]], size=2) +
  xlab("X") + ylab("Y")

Sys.sleep(2)

ggplot(data = NULL) +
  geom_line(aes(x=1:numIterations, y=list[[1]])) +
  xlab("# of Iterations") + ylab("Cost")

costList = vector()

for(i in 2:10){
  costList[i] = kmeans_simulation(numPoints, i, numIterations, numClusters.data, spread)[[1]][numIterations]
}

ggplot(data = NULL) +
  geom_line(aes(x = 1:length(costList), y = costList)) +
  xlab("# of Clusters") + ylab("Cost")

```



#Conclusion
Thank you for taking the time to read this explanation for the "K Means Clustering Algorithm". For more information about the algorithm, please visit this link. Link: https://projects.iq.harvard.edu/files/stat110/files/strategic_practice_and_homework_3.pdf.  This explanation and the included code is **100% my original work**. Please feel free to visit my GitHub page at http://github.com/goutham1220 where I will be posting more explanations as well as other statistics and data science-related resources. In addition, please feel free to visit my YouTube channels "GSDataScience" (http://bit.ly/gsdatascience), where I will be posting more data science and statistics-related videos, and "Gooth" (http://youtube.com/gooth), where I post more cinematic-style, slice-of-life videos.    

